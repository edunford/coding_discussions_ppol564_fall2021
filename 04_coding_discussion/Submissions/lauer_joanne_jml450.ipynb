{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Discussion 4\n",
    "### Joanne Lauer\n",
    "### jml450\n",
    "### November 7, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening all articles\n",
    "filebbc = open('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/bbc-khashoggi.txt')\n",
    "articlebbc=filebbc.read().replace( \"\\n\", \" \")\n",
    "filebbc.close()\n",
    "filebb = open ('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/breitbart-khashoggi.txt')\n",
    "articlebb=filebb.read().replace( \"\\n\", \" \")\n",
    "filebb.close()\n",
    "filecnn = open ('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/cnn-khashoggi.txt') \n",
    "articlecnn=filecnn.read().replace( \"\\n\", \" \")\n",
    "filecnn.close()\n",
    "filefox = open ('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/fox-khashoggi.txt')\n",
    "articlefox=filefox.read().replace( \"\\n\", \" \")\n",
    "filefox.close()\n",
    "fileal = open ('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/aljazeera-khashoggi.txt')\n",
    "articleal=fileal.read().replace( \"\\n\", \" \")\n",
    "fileal.close()\n",
    "# read in stop file\n",
    "stop_file = pd.read_csv ('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/stop_words.csv')\n",
    "# convert df to list \n",
    "# stop_list_full = stop_file.values.tolist\n",
    "stop_list_full = stop_file['word'].to_list()\n",
    "# read in stop file that has been limited to just a few words and numbers.  \n",
    "stop_file_reduced = pd.read_csv ('/Users/joannespringer/coding_discussions_ppol564_fall2021/04_coding_discussion/Data/stop_words_2.csv')\n",
    "# convert df to list\n",
    "stop_list_reduced = stop_file_reduced['word'].to_list()\n",
    "# print statement used for trouble shooting prints out all articles\n",
    "# print(articlebbc,articlebb,articlecnn,articlefox,articleal)\n",
    "# print statement used fro trouble shooting prints out csv file(s)\n",
    "# print(stop_file)\n",
    "# print(stop_file_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the string, use a regular expression to remove the punctuation and numbers, then split \n",
    "# the string, set the upper to lower\n",
    "def tokenize(text=None):\n",
    "    \"\"\"\n",
    "    Accept a string of text that is then separated and retruned in all lowercas minus punctuation and numbers.\n",
    "    \n",
    "    Arguments:\n",
    "    __________\n",
    "    text = a string of text\n",
    "\n",
    "    Return:\n",
    "    ______\n",
    "    text_list = a list of converted text which has been converted to lowercase with no punctuation or numbers, \n",
    "    split into words\n",
    "    \"\"\"\n",
    "    # set words to lower \n",
    "    text = text.lower()\n",
    "    # use regular expression to remove special characters\n",
    "    text = re.sub(r'[\\W\\d]',' ',text)\n",
    "    # split the string into individual words\n",
    "    text_list = text.split()\n",
    "    # return the list \n",
    "    return text_list        \n",
    "# tokenize returns a simple version of the string provided\n",
    "bbc_0 = tokenize(articlebbc)\n",
    "bb_0 = tokenize(articlebb)\n",
    "cnn_0 = tokenize(articlecnn)\n",
    "fox_0 = tokenize(articlefox)\n",
    "al_0 = tokenize(articleal)\n",
    "# Print statements used for trouble shooting\n",
    "# print(tokenize(articlebbc))\n",
    "# print(tokenize(articlebb))\n",
    "# print(tokenize(articlecnn))\n",
    "# print(tokenize(articlefox))\n",
    "# print(tokenize(articleal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the string, use a regular expression to remove the punctuation, then split the string, set the upper to lower\n",
    "# and remove the words in the stop file. \n",
    "def tokenize(text=None):\n",
    "    \"\"\"\n",
    "    Accept a string of text that is then separated and retruned in all lowercas minus punctuation, numbers, and words in the \n",
    "    reduced stop file.\n",
    "    \n",
    "    Arguments:\n",
    "    __________\n",
    "    text = a string of text\n",
    "\n",
    "    Return:\n",
    "    ______\n",
    "    text_list_clean_reduced = a list of converted text which has been converted to lowercase with no punctuation, split into\n",
    "    words and has has all words from the stop_file removed\n",
    "    \"\"\"\n",
    "    # set words to lower \n",
    "    text = text.lower()\n",
    "    # use regular expression to remove special characters\n",
    "    text = re.sub(r'[\\W\\d]',' ',text)\n",
    "    # split the string into individual words\n",
    "    text_list = text.split()\n",
    "    # create an empty list to hold the clean list after comparing to the stop file\n",
    "    text_list_clean_reduced = [word for word in text_list if word not in stop_list_reduced]\n",
    "    # return the list \n",
    "    return text_list_clean_reduced        \n",
    "# tokenize returns a simple version of the string provided\n",
    "bbc_1 = tokenize(articlebbc)\n",
    "bb_1 = tokenize(articlebb)\n",
    "cnn_1 = tokenize(articlecnn)\n",
    "fox_1 = tokenize(articlefox)\n",
    "al_1 = tokenize(articleal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the string, use a regular expression to remove the punctuation, then split the string, set the upper to lower\n",
    "# and remove the words in the stop file. \n",
    "def tokenize(text=None):\n",
    "    \"\"\"\n",
    "    Accept a string of text that is then separated and retruned in all lowercas minus punctuation and words in the \n",
    "    stop file.\n",
    "    \n",
    "    Arguments:\n",
    "    __________\n",
    "    text = a string of text\n",
    "\n",
    "    Return:\n",
    "    ______\n",
    "    text_list_clean = a list of converted text which has been converted to lowercase with no punctuation, split into\n",
    "    words and has has all words from the stop_file removed\n",
    "    \"\"\"\n",
    "    #'[\\d,\\.\\?:;\\\"\\'!\\(\\)\\$\\-]*',''\n",
    "    # set words to lower \n",
    "    text = text.lower()\n",
    "    # use regular expression to remove special characters\n",
    "    text = re.sub(r'[\\W]',' ',text)\n",
    "    # split the string into individual words\n",
    "    text_list = text.split()\n",
    "    # create an empty list to hold the clean list after comparing to the stop file\n",
    "    text_list_clean_full = [word for word in text_list if word not in stop_list_full]\n",
    "    # return the list \n",
    "    return text_list_clean_full        \n",
    "# tokenize returns a simple version of the string provided\n",
    "bbc_2 = tokenize(articlebbc)\n",
    "bb_2 = tokenize(articlebb)\n",
    "cnn_2 = tokenize(articlecnn)\n",
    "fox_2 = tokenize(articlefox)\n",
    "al_2 = tokenize(articleal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turkey</th>\n",
       "      <th>istanbul</th>\n",
       "      <th>turkish</th>\n",
       "      <th>president</th>\n",
       "      <th>recep</th>\n",
       "      <th>tayyip</th>\n",
       "      <th>erdogan</th>\n",
       "      <th>has</th>\n",
       "      <th>said</th>\n",
       "      <th>murder</th>\n",
       "      <th>...</th>\n",
       "      <th>thought</th>\n",
       "      <th>saudis</th>\n",
       "      <th>know</th>\n",
       "      <th>very</th>\n",
       "      <th>well</th>\n",
       "      <th>knows</th>\n",
       "      <th>doing</th>\n",
       "      <th>namely</th>\n",
       "      <th>asking</th>\n",
       "      <th>cooperation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   turkey  istanbul  turkish  president  recep  tayyip  erdogan  has  said  \\\n",
       "0       4         5        7          4      1       1       14    6     6   \n",
       "\n",
       "   murder  ...  thought  saudis  know  very  well  knows  doing  namely  \\\n",
       "0       6  ...        1       2     1     1     1      1      1       1   \n",
       "\n",
       "   asking  cooperation  \n",
       "0       1            1  \n",
       "\n",
       "[1 rows x 263 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to create a datafram out of a list of strings\n",
    "def convert_text_to_dtm(txt):\n",
    "    \"\"\"\n",
    "    Accepts a list of strings that was created by tokenize and puts it into a document term matrix    \n",
    "    Arguments:\n",
    "    __________\n",
    "    txt = list of strings\n",
    "\n",
    "    Returns:\n",
    "    ______\n",
    "    pd.DataFrame(d) = a docment term matrix\n",
    "    \"\"\"\n",
    "   # Converts text into a document term matrix\n",
    "    \n",
    "    d = dict()\n",
    "    for word in txt:\n",
    "        if word in d:\n",
    "            d[word][0] += 1\n",
    "        else:\n",
    "            d[word] = [1]\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "convert_text_to_dtm(bbc_0)\n",
    "convert_text_to_dtm(bbc_1)\n",
    "convert_text_to_dtm(bbc_2)\n",
    "convert_text_to_dtm(bb_0)\n",
    "convert_text_to_dtm(bb_1)\n",
    "convert_text_to_dtm(bb_1)\n",
    "convert_text_to_dtm(cnn_0)\n",
    "convert_text_to_dtm(cnn_1)\n",
    "convert_text_to_dtm(cnn_2)\n",
    "convert_text_to_dtm(fox_0)\n",
    "convert_text_to_dtm(fox_1)\n",
    "convert_text_to_dtm(fox_1)\n",
    "convert_text_to_dtm(al_0)\n",
    "convert_text_to_dtm(al_1)\n",
    "convert_text_to_dtm(al_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   u  s  president  donald  trump  and  turkish  recep  tayyip  erdogan  ...  \\\n",
      "0  2  7          5       1      4    8        6      1       1        4  ...   \n",
      "\n",
      "   its  authenticity  deliberately  withheld  top  so  will  placed  \\\n",
      "0    1             1             1         1    1   1     1       1   \n",
      "\n",
      "   uncomfortable  diplomatic  \n",
      "0              1           1  \n",
      "\n",
      "[1 rows x 278 columns]\n",
      "   u  s  president  donald  trump  and  turkish  recep  tayyip  erdogan  ...  \\\n",
      "0  2  7          5       1      4    8        6      1       1        4  ...   \n",
      "\n",
      "   its  authenticity  deliberately  withheld  top  so  will  placed  \\\n",
      "0    1             1             1         1    1   1     1       1   \n",
      "\n",
      "   uncomfortable  diplomatic  \n",
      "0              1           1  \n",
      "\n",
      "[1 rows x 278 columns]\n",
      "   istanbul  turkey  cnn  jamal  khashoggi  died  as   a  result  of  ...  \\\n",
      "0         5       4    1      1          6     2   2  14       2   6  ...   \n",
      "\n",
      "   locations  were  scouted  but  noted  later  body  yet  read  more  \n",
      "0          1     1        1    1      1      1     1    1     1     1  \n",
      "\n",
      "[1 rows x 144 columns]\n",
      "   saudi  officials  planned  the  savage  murder  of  writer  jamal  \\\n",
      "0      9          2        2   42       3       6  18       2      3   \n",
      "\n",
      "   khashoggi  ...  leaving  rally  texas  fox  greg  palkot  associated  \\\n",
      "0         12  ...        1      1      1    1     1       1           1   \n",
      "\n",
      "   press  contributed  report  \n",
      "0      1            1       1  \n",
      "\n",
      "[1 rows x 251 columns]\n",
      "   turkey  istanbul  turkish  president  recep  tayyip  erdogan  has  said  \\\n",
      "0       4         5        7          4      1       1       14    6     6   \n",
      "\n",
      "   the  ...  thought  saudis  know  very  well  knows  doing  namely  asking  \\\n",
      "0   38  ...        1       2     1     1     1      1      1       1       1   \n",
      "\n",
      "   cooperation  \n",
      "0            1  \n",
      "\n",
      "[1 rows x 284 columns]\n",
      "   recep  tayyip  erdogan  says  many  questions  need  answered  killing  \\\n",
      "0      2       2       11     4     4          1     1         1        7   \n",
      "\n",
      "   journalist  ...  laid  investments  turn  sour  despite  mood  optimism  \\\n",
      "0           1  ...     1            1     1     1        1     1         1   \n",
      "\n",
      "   self  approbation  exhibited  \n",
      "0     1            1          1  \n",
      "\n",
      "[1 rows x 425 columns]\n",
      "   president  donald  trump  turkish  recep  tayyip  erdogan  both  \\\n",
      "0          5       1      4        6      1       1        4     2   \n",
      "\n",
      "   pronounced  themselves  ...  yet  confirmed  authenticity  deliberately  \\\n",
      "0           1           1  ...    1          1             1             1   \n",
      "\n",
      "   withheld  top  will  placed  uncomfortable  diplomatic  \n",
      "0         1    1     1       1              1           1  \n",
      "\n",
      "[1 rows x 248 columns]\n",
      "   istanbul  turkey  cnn  jamal  khashoggi  died  as  result  brutal  \\\n",
      "0         5       4    1      1          6     2   2       2       1   \n",
      "\n",
      "   premeditated  ...  locations  were  scouted  but  noted  later  body  yet  \\\n",
      "0             1  ...          1     1        1    1      1      1     1    1   \n",
      "\n",
      "   read  more  \n",
      "0     1     1  \n",
      "\n",
      "[1 rows x 130 columns]\n",
      "   saudi  officials  planned  savage  murder  writer  jamal  khashoggi  days  \\\n",
      "0      9          2        2       3       6       2      3         12     1   \n",
      "\n",
      "   before  ...  leaving  rally  texas  fox  greg  palkot  associated  press  \\\n",
      "0       3  ...        1      1      1    1     1       1           1      1   \n",
      "\n",
      "   contributed  report  \n",
      "0            1       1  \n",
      "\n",
      "[1 rows x 221 columns]\n",
      "   turkey  istanbul  turkish  president  recep  tayyip  erdogan  has  said  \\\n",
      "0       4         5        7          4      1       1       14    6     6   \n",
      "\n",
      "   murder  ...  thought  saudis  know  very  well  knows  doing  namely  \\\n",
      "0       6  ...        1       2     1     1     1      1      1       1   \n",
      "\n",
      "   asking  cooperation  \n",
      "0       1            1  \n",
      "\n",
      "[1 rows x 263 columns]\n",
      "   recep  tayyip  erdogan  questions  answered  killing  journalist  jamal  \\\n",
      "0      2       2       11          1         1        7           1      5   \n",
      "\n",
      "   khashoggi  planned  ...  confidence  vision  future  laid  investments  \\\n",
      "0         16        1  ...           1       1       2     1            1   \n",
      "\n",
      "   sour  mood  optimism  approbation  exhibited  \n",
      "0     1     1         1            1          1  \n",
      "\n",
      "[1 rows x 328 columns]\n",
      "   president  donald  trump  turkish  recep  tayyip  erdogan  pronounced  \\\n",
      "0          5       1      4        6      1       1        4           1   \n",
      "\n",
      "   unsatisfied  answers  ...  anonymous  cia  officials  confirmed  \\\n",
      "0            1        2  ...          1    1          2          1   \n",
      "\n",
      "   authenticity  deliberately  withheld  top  uncomfortable  diplomatic  \n",
      "0             1             1         1    1              1           1  \n",
      "\n",
      "[1 rows x 174 columns]\n",
      "   istanbul  turkey  cnn  jamal  khashoggi  died  result  brutal  \\\n",
      "0         5       4    1      1          6     2       2       1   \n",
      "\n",
      "   premeditated  murder  ...  mile  90  kilometer  drive  south  locations  \\\n",
      "0             1       1  ...     1   1          1      1      1          1   \n",
      "\n",
      "   scouted  noted  body  read  \n",
      "0        1      1     1     1  \n",
      "\n",
      "[1 rows x 100 columns]\n",
      "   saudi  officials  planned  savage  murder  writer  jamal  khashoggi  days  \\\n",
      "0      9          2        2       3       6       2      3         12     1   \n",
      "\n",
      "   death  ...  media  leaving  rally  texas  fox  greg  palkot  press  \\\n",
      "0      3  ...      1        1      1      1    1     1       1      1   \n",
      "\n",
      "   contributed  report  \n",
      "0            1       1  \n",
      "\n",
      "[1 rows x 163 columns]\n",
      "   turkey  istanbul  turkish  president  recep  tayyip  erdogan  murder  \\\n",
      "0       4         5        7          4      1       1       14       6   \n",
      "\n",
      "   journalist  jamal  ...  jubeir  terrible  tragedy  taha  ozhan  research  \\\n",
      "0           1      1  ...       1         1        1     1      1         1   \n",
      "\n",
      "   director  institute  saudis  cooperation  \n",
      "0         1          1       2            1  \n",
      "\n",
      "[1 rows x 186 columns]\n"
     ]
    }
   ],
   "source": [
    "# a function to create a dataframe of dataframes and fill in missing values with 0's \n",
    "def gen_DTM(texts=None):\n",
    "    \"\"\"\n",
    "    Accepts the dataframe created in convert_text_to_dtm and places them in one data term matrix and fills in \n",
    "    missing numbers with 0's\n",
    "    \n",
    "    Arguments:\n",
    "    __________\n",
    "    texts = data frame\n",
    "    \n",
    "    Returns:\n",
    "    ______\n",
    "    pd.DataFrame(d) = a docment term matrix\n",
    "    \"\"\"\n",
    "    # Generate a document term matrix\n",
    "    \n",
    "    DTM = pd.DataFrame()\n",
    "    for detail in texts:\n",
    "        entry = (detail)\n",
    "        print(entry)\n",
    "        DTM = DTM.append(pd.DataFrame(entry),ignore_index=True,sort=True) # Row bind\n",
    "    \n",
    "    # Fill in any missing values with 0s\n",
    "    DTM.fillna(0, inplace=True) \n",
    "    return DTM\n",
    "       \n",
    "genDTM0 = gen_DTM([(convert_text_to_dtm(bb_0)),\n",
    "    (convert_text_to_dtm(bb_0)),\n",
    "    (convert_text_to_dtm(cnn_0)),\n",
    "    (convert_text_to_dtm(fox_0)),\n",
    "    (convert_text_to_dtm(al_0))])\n",
    "genDTM1 = gen_DTM([(convert_text_to_dtm(bbc_1)),\n",
    "    (convert_text_to_dtm(bb_1)),(convert_text_to_dtm(cnn_1)),\n",
    "    (convert_text_to_dtm(fox_1)),(convert_text_to_dtm(al_1))])    \n",
    "genDTM2 = gen_DTM([(convert_text_to_dtm(bbc_2)),\n",
    "    (convert_text_to_dtm(bb_2)),(convert_text_to_dtm(cnn_2)),\n",
    "    (convert_text_to_dtm(fox_2)),(convert_text_to_dtm(al_2))])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break out each dataframe from the dataframe so they can be easily accessed\n",
    "bbc_d0 = genDTM0.iloc[0].values\n",
    "bb_d0 = genDTM0.iloc[1].values\n",
    "cnn_d0 = genDTM0.iloc[2].values\n",
    "fox_d0 = genDTM0.iloc[3].values\n",
    "al_d0 = genDTM0.iloc[4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break out each dataframe from the dataframe so they can be easily accessed\n",
    "\n",
    "bbc_d1 = genDTM1.iloc[0].values\n",
    "bb_d1 = genDTM1.iloc[1].values\n",
    "cnn_d1 = genDTM1.iloc[2].values\n",
    "fox_d1 = genDTM1.iloc[3].values\n",
    "al_d1 = genDTM1.iloc[4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break out each dataframe from the dataframe so they can be easily accessed\n",
    "\n",
    "bbc_d2 = genDTM2.iloc[0].values\n",
    "bb_d2 = genDTM2.iloc[1].values\n",
    "cnn_d2 = genDTM2.iloc[2].values\n",
    "fox_d2 = genDTM2.iloc[3].values\n",
    "al_d2 = genDTM2.iloc[4].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7038\n",
      "0.8693\n",
      "0.8412\n",
      "0.7038\n",
      "0.8693\n",
      "0.8412\n",
      "0.7686\n",
      "0.7654\n",
      "0.8476\n",
      "Only punctuation and numbers removed\n",
      "BBC and CNN\n",
      "0.7038\n",
      "Breithart and CNN\n",
      "0.7038\n",
      "CNN and Aljazeera\n",
      "0.7654\n",
      "CNN and Fox\n",
      "0.7686\n",
      "BBC and Aljazeera\n",
      "0.8412\n",
      "Breithart and CNN\n",
      "0.8412\n",
      "FOX and Aljazeera\n",
      "0.8476\n",
      "BBC and Fox\n",
      "0.8693\n",
      "Breithart and FOX\n",
      "0.8693\n",
      "BBC and Breitbart\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#function to create the cosine\n",
    "def cosine(a,b):\n",
    "    cos = round(np.dot(a,b)/(np.sqrt(np.dot(a,a)) * np.sqrt(np.dot(b,b))),4)\n",
    "    print(cos)\n",
    "    return cos\n",
    "    \n",
    "\n",
    "BBC_Breitbart =cosine(bbc_d0,bb_d0)\n",
    "BBC_CNN = cosine(bbc_d0,cnn_d0)\n",
    "BBC_FOX = cosine(bbc_d0,fox_d0)\n",
    "BBC_Aljazeera = cosine(bbc_d0,al_d0)\n",
    "Breitbart_CNN = cosine(bb_d0,cnn_d0)\n",
    "Breitbart_FOX = cosine(bb_d0,fox_d0)\n",
    "Breitbart_Aljazeera = cosine(bb_d0,al_d0)\n",
    "CNN_FOX = cosine(cnn_d0,fox_d0)\n",
    "CNN_Aljazeera = cosine(cnn_d0,al_d0)\n",
    "FOX_Aljazeera = cosine(fox_d0,al_d0)\n",
    "\n",
    "cos_list_0 = [(\"BBC and Breitbart\", BBC_Breitbart),(\"BBC and CNN\", BBC_CNN),(\"BBC and Fox\",BBC_FOX),(\"BBC and Aljazeera\",BBC_Aljazeera),(\"Breithart and CNN\",Breitbart_CNN),(\"Breithart and FOX\",Breitbart_FOX),(\"Breithart and CNN\",Breitbart_Aljazeera),(\"CNN and Fox\",CNN_FOX),(\"CNN and Aljazeera\",CNN_Aljazeera),(\"FOX and Aljazeera\",FOX_Aljazeera)]\n",
    "print(\"Only punctuation and numbers removed\") \n",
    "cos_list_0.sort(key=lambda y: y[1])\n",
    "for elem in (np.concatenate(cos_list_0)):\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662\n",
      "0.6495\n",
      "0.695\n",
      "0.7176\n",
      "0.5009\n",
      "0.5971\n",
      "0.6435\n",
      "0.6132\n",
      "0.6087\n",
      "0.692\n",
      "Few Common Words Removed\n",
      "Breithart and CNN\n",
      "0.5009\n",
      "Breithart and FOX\n",
      "0.5971\n",
      "CNN and Aljazeera\n",
      "0.6087\n",
      "CNN and Fox\n",
      "0.6132\n",
      "Breithart and CNN\n",
      "0.6435\n",
      "BBC and CNN\n",
      "0.6495\n",
      "BBC and Breitbart\n",
      "0.662\n",
      "FOX and Aljazeera\n",
      "0.692\n",
      "BBC and Fox\n",
      "0.695\n",
      "BBC and Aljazeera\n",
      "0.7176\n"
     ]
    }
   ],
   "source": [
    "#function to create the cosine\n",
    "def cosine(a,b):\n",
    "    cos = round(np.dot(a,b)/(np.sqrt(np.dot(a,a)) * np.sqrt(np.dot(b,b))),4)\n",
    "    print(cos)\n",
    "    return cos\n",
    "    \n",
    "\n",
    "BBC_Breitbart =cosine(bbc_d1,bb_d1)\n",
    "BBC_CNN = cosine(bbc_d1,cnn_d1)\n",
    "BBC_FOX = cosine(bbc_d1,fox_d1)\n",
    "BBC_Aljazeera = cosine(bbc_d1,al_d1)\n",
    "Breitbart_CNN = cosine(bb_d1,cnn_d1)\n",
    "Breitbart_FOX = cosine(bb_d1,fox_d1)\n",
    "Breitbart_Aljazeera = cosine(bb_d1,al_d1)\n",
    "CNN_FOX = cosine(cnn_d1,fox_d1)\n",
    "CNN_Aljazeera = cosine(cnn_d1,al_d1)\n",
    "FOX_Aljazeera = cosine(fox_d1,al_d1)\n",
    "\n",
    "cos_list_1 = [(\"BBC and Breitbart\", BBC_Breitbart),(\"BBC and CNN\", BBC_CNN),(\"BBC and Fox\",BBC_FOX),(\"BBC and Aljazeera\",BBC_Aljazeera),(\"Breithart and CNN\",Breitbart_CNN),(\"Breithart and FOX\",Breitbart_FOX),(\"Breithart and CNN\",Breitbart_Aljazeera),(\"CNN and Fox\",CNN_FOX),(\"CNN and Aljazeera\",CNN_Aljazeera),(\"FOX and Aljazeera\",FOX_Aljazeera)]\n",
    "print(\"Few Common Words Removed\") \n",
    "cos_list_1.sort(key=lambda y: y[1])\n",
    "for elem in (np.concatenate(cos_list_1)):\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6209\n",
      "0.5505\n",
      "0.6907\n",
      "0.7041\n",
      "0.422\n",
      "0.5788\n",
      "0.5997\n",
      "0.6026\n",
      "0.6075\n",
      "0.7171\n",
      "Most Common Words Removed\n",
      "Breithart and CNN\n",
      "0.422\n",
      "BBC and CNN\n",
      "0.5505\n",
      "Breithart and FOX\n",
      "0.5788\n",
      "Breithart and CNN\n",
      "0.5997\n",
      "CNN and Fox\n",
      "0.6026\n",
      "CNN and Aljazeera\n",
      "0.6075\n",
      "BBC and Breitbart\n",
      "0.6209\n",
      "BBC and Fox\n",
      "0.6907\n",
      "BBC and Aljazeera\n",
      "0.7041\n",
      "FOX and Aljazeera\n",
      "0.7171\n"
     ]
    }
   ],
   "source": [
    "#function to create the cosine\n",
    "def cosine(a,b):\n",
    "    cos = round(np.dot(a,b)/(np.sqrt(np.dot(a,a)) * np.sqrt(np.dot(b,b))),4)\n",
    "    print(cos)\n",
    "    return cos\n",
    "    \n",
    "\n",
    "BBC_Breitbart =cosine(bbc_d2,bb_d2)\n",
    "BBC_CNN = cosine(bbc_d2,cnn_d2)\n",
    "BBC_FOX = cosine(bbc_d2,fox_d2)\n",
    "BBC_Aljazeera = cosine(bbc_d2,al_d2)\n",
    "Breitbart_CNN = cosine(bb_d2,cnn_d2)\n",
    "Breitbart_FOX = cosine(bb_d2,fox_d2)\n",
    "Breitbart_Aljazeera = cosine(bb_d2,al_d2)\n",
    "CNN_FOX = cosine(cnn_d2,fox_d2)\n",
    "CNN_Aljazeera = cosine(cnn_d2,al_d2)\n",
    "FOX_Aljazeera = cosine(fox_d2,al_d2)\n",
    "\n",
    "cos_list_2 = [(\"BBC and Breitbart\", BBC_Breitbart),(\"BBC and CNN\", BBC_CNN),(\"BBC and Fox\",BBC_FOX),(\"BBC and Aljazeera\",BBC_Aljazeera),(\"Breithart and CNN\",Breitbart_CNN),(\"Breithart and FOX\",Breitbart_FOX),(\"Breithart and CNN\",Breitbart_Aljazeera),(\"CNN and Fox\",CNN_FOX),(\"CNN and Aljazeera\",CNN_Aljazeera),(\"FOX and Aljazeera\",FOX_Aljazeera)]\n",
    " \n",
    "cos_list_2.sort(key=lambda y: y[1])\n",
    "print(\"Most Common Words Removed\")\n",
    "for elem in (np.concatenate(cos_list_2)):\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of words removed seemed to matter much less than removing common punctuation and numbers.  I was \n",
    "### very surprised that the lowest match of Breitbart and CNN and the largest match of Fox an Aljazeera didn't change.\n",
    "### What was most surprising was that BBC and Breitbart had an exact match with only punctuation and numbers removed."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
