{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Discussion 05\n",
    "\n",
    "### Can we predict whether someone will vote or not?\n",
    "\n",
    "• The data is drawn from the 2012 National Election Survey and records the age, eduction level (of total years in school), income, race (caucasian or not), and past voting record (i.e. whether or not the respondent voted in the 2012 Presidential election). The sample is composed of 2000 individual respondents.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "• Please break the data up into a training (1600 entries, 80%) and test dataset (400 entries, 20%).\n",
    "\n",
    "• Build a Naive Bayesian Classifier from scratch that tries to predict whether a respondent will vote in a presidential election or not, pr(Vote==1). The classifier must be built from scratch. Do not use a third party ML or statistical package.\n",
    "\n",
    "• Run your algorithm and see how it predicts on the test data by calculating the predictive accuracy.\n",
    "\n",
    "• Does your model perform better than chance (i.e. coin flip)?\n",
    "\n",
    "• When completing this answer, be sure to: 1.) comment on all of your code 2.) provide a narrative for what you're doing and 3.) summarize your results and findings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp # for printing\n",
    "import scipy.stats as st # for Normal PDF\n",
    "import requests\n",
    "\n",
    "# Plotting libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "\n",
    "# Silence warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file from github \n",
    "url = 'https://raw.githubusercontent.com/edunford/coding_discussions_ppol564_fall2021/main/05_coding_discussion/turnout.csv'\n",
    "res = requests.get(url, allow_redirects=True)\n",
    "with open('turnout.csv','wb') as file:\n",
    "    file.write(res.content)\n",
    "turnout = pd.read_csv('turnout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 1600 \n",
      "Test Data: 400\n"
     ]
    }
   ],
   "source": [
    "# Train-Test split (just using Pandas)\n",
    "train = turnout.sample(frac=.8).reset_index(drop=True)\n",
    "test = turnout.drop(train.index).reset_index(drop=True)\n",
    "# Print off the split count \n",
    "print(\"Training Data:\",train.shape[0],\n",
    "      \"\\nTest Data:\",test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop, rearrange, and rename data \n",
    "train_binary = train.drop(columns=['id', 'age', 'educate', 'income'])\n",
    "y, x1 = train_binary.iloc[1,:]\n",
    "test_binary = test.drop(columns=['id', 'age', 'educate', 'income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$Pr(class | data) = Pr( x_1| class)\\times Pr( x_2| class) \\times \\dots \\times  Pr(class)$$\n",
    "\n",
    "A Naive Bayes simplifies a traditional Bayesian classifier by assuming each variable is independent of each other and removing the normalizing factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> Naive Bayesian Classifier with Binary Predictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to calculate each component of the equation above. First, let's determine the $Pr(class)$ or the fraction of people who voted and people who did not vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pr(vote = 1): 0.74375\n",
      "Pr(vote = 0): 0.25625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = train.shape[0]\n",
    "\n",
    "# Subset the data by class\n",
    "vote1 = train.query(\"vote == 1\")\n",
    "vote0 = train.query(\"vote == 0\")\n",
    "\n",
    "# Calculate the probability for each class\n",
    "pr_vote_1 = vote1.shape[0]/N\n",
    "pr_vote_0 = vote0.shape[0]/N\n",
    "\n",
    "# Print the probabilities\n",
    "print(\n",
    "f\"\"\"\n",
    "Pr(vote = 1): {pr_vote_1}\n",
    "Pr(vote = 0): {pr_vote_0}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to calculate the conditional probabilities or $Pr(data | class)$. In other words, from the training set, let's find the number of white individuals who voted and did not vote and non-white individuals who voted and did not vote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pr(white = 1 |vote = 1): 0.8815126050420168\n",
      "Pr(white = 0 |vote = 1): 0.11848739495798319\n",
      "Pr(white = 1 |vote = 0): 0.775609756097561\n",
      "Pr(white = 0 |vote = 0): 0.22439024390243903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given vote == 1\n",
    "w1_vote1 = vote1.query(\"white == 1\").shape[0]/vote1.shape[0]\n",
    "w0_vote1 = vote1.query(\"white == 0\").shape[0]/vote1.shape[0]\n",
    "\n",
    "\n",
    "# Given vote == 0\n",
    "w1_vote0 = vote0.query(\"white == 1\").shape[0]/vote0.shape[0]\n",
    "w0_vote0 = vote0.query(\"white == 0\").shape[0]/vote0.shape[0]\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "Pr(white = 1 |vote = 1): {w1_vote1}\n",
    "Pr(white = 0 |vote = 1): {w0_vote1}\n",
    "Pr(white = 1 |vote = 0): {w1_vote0}\n",
    "Pr(white = 0 |vote = 0): {w0_vote0}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pr(vote = 1) = 0.088125\n",
      "Pr(vote = 0) = 0.057499999999999996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_vote1 = w0_vote1 * pr_vote_1\n",
    "prob_vote0 = w0_vote0 * pr_vote_0\n",
    "\n",
    "print(f\"\"\"\n",
    "Pr(vote = 1) = {prob_vote1}\n",
    "Pr(vote = 0) = {prob_vote0}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pr(cw = 1) = 0.655625\n",
      "Pr(cw = 0) = 0.19874999999999998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_vote1 = w1_vote1 * pr_vote_1\n",
    "prob_vote0 = w1_vote0 * pr_vote_0\n",
    "\n",
    "print(f\"\"\"\n",
    "Pr(cw = 1) = {prob_vote1}\n",
    "Pr(cw = 0) = {prob_vote0}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the underlying probabilities and then calculate the predictions for each observation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class probabilities\n",
      "\n",
      "{0: 0.25625, 1: 0.74375}\n"
     ]
    }
   ],
   "source": [
    "def calc_probs(data,outcome_var=\"\"):\n",
    "    '''\n",
    "    Function calculates the class and conditional probabilities in \n",
    "    the binary data. \n",
    "    \n",
    "    Note that I'm using dictionaries with tuple keys to keep\n",
    "    track of the variable, it's val, and the outcome, which we're conditioning on. \n",
    "    '''\n",
    "    # Generate empty dictionary containers.\n",
    "    class_probs = {};cond_probs = {}\n",
    "    # Locate all variables that are not the outcome.\n",
    "    vars = [v for v in data.columns if v != outcome_var]\n",
    "    # iterate through the class outcomes\n",
    "    for y, d in data.groupby(outcome_var): \n",
    "        # calculate the class probabilities\n",
    "        class_probs.update({y: d.shape[0]/data.shape[0]})\n",
    "        for v in vars:\n",
    "            # calculate the conditional probabilities for each variable given the class.\n",
    "            pr = d[v].sum()/d.shape[0]\n",
    "            cond_probs[(v,1,y)] = pr \n",
    "            cond_probs[(v,0,y)] = 1 - pr\n",
    "    return class_probs, cond_probs\n",
    "\n",
    "\n",
    "# Run\n",
    "class_probs, cond_probs = calc_probs(train_binary,outcome_var=\"vote\")\n",
    "\n",
    "# Print\n",
    "print(\"class probabilities\",end=\"\\n\\n\")\n",
    "pp.pprint(class_probs)\n",
    "#print(\"\\n\")\n",
    "#print(\"conditional probabilities\",end=\"\\n\\n\")\n",
    "#pp.pprint(cond_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a prediction function that runs through the observations in the data and calculates the probabilities and makes a class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,class_probs,cond_probs):\n",
    "    '''\n",
    "    Function calculates the conditiona probability for membership into each class.\n",
    "    Then returns both the probabilities and the most likely class. \n",
    "    '''\n",
    "    store_preds = []\n",
    "    for i,row in data.iterrows():\n",
    "        pr_1 = 1; pr_0 = 1\n",
    "        for j in range(1,len(row.index)):\n",
    "            pr_0 *= cond_probs[(row.index[j],row.values[j],0)]\n",
    "            pr_1 *= cond_probs[(row.index[j],row.values[j],1)]     \n",
    "        pr_0 *= class_probs[0]\n",
    "        pr_1 *= class_probs[1]\n",
    "        store_preds.append([pr_0,pr_1,max([(pr_0,0),(pr_1,1)])[1]])\n",
    "    return pd.DataFrame(store_preds,columns=[\"pr_0\",\"pr_1\",\"pred\"])\n",
    "\n",
    "# Run \n",
    "preds = predict(train_binary, class_probs, cond_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculate the predictive accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74375"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(train_binary.vote == preds.pred)/train.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's the accuracy of the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = predict(test_binary, class_probs, cond_probs)\n",
    "test_accuracy = sum(test_binary.vote == test_preds.pred)/test.shape[0]\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the Naive Bayes Classifier is quite simplistic when compared to other modeling strategies (such as a neural net or a gradient boosting machine), it proves to be effective on a wide array of prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> Naive Bayesian Classifier with Continuous Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to map a continuous variable into a probability space. Here we'll use the probability density function for Gaussian (normal) distribution to convert continuous values into probabilities.\n",
    "\n",
    "Note that we can use information regarding the distribution of each continuous predict and find out where any single point is on that continuous variables probability distribution.\n",
    "\n",
    "We can calculate the conditional mean and standard deviation for each value of the outcome and then calculate the predictions from there for any one of our continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop, rearrange, and rename train df \n",
    "train_drop = train.drop(columns=['id', 'white', 'age'])\n",
    "train_drop = train_drop[['vote', 'educate', 'income']]\n",
    "y,x1,x2 = train_drop.iloc[1,:]\n",
    "train_drop.columns = ['y', 'x1', 'x2']\n",
    "\n",
    "# Drop, rearrange, and rename test df \n",
    "test_drop = test.drop(columns=['id', 'white', 'age'])\n",
    "test_drop = test_drop[['vote', 'educate', 'income']]\n",
    "test_drop.columns = ['y', 'x1', 'x2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the class probabilities or $Pr(class)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = train_drop.query(\"y == 1\")\n",
    "y0 = train_drop.query(\"y == 0\")\n",
    "\n",
    "# Class probabilities.\n",
    "pr_y1 = y1.shape[0]/train_drop.shape[0]\n",
    "pr_y0 = y0.shape[0]/train_drop.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the means and standard devitaions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('x1', 0): {'mean': 2.8572895261845397, 'sd': 3.263255626180008},\n",
      " ('x1', 1): {'mean': 4.226724103419518, 'sd': 3.297833599903187},\n",
      " ('x2', 0): {'mean': 10.688279301745636, 'sd': 2.3290857540116523},\n",
      " ('x2', 1): {'mean': 12.51834862385321, 'sd': 2.901260701952806}}\n"
     ]
    }
   ],
   "source": [
    "# Collect the mean and standard dev. of each conditional distribution\n",
    "dist_locs = \\\n",
    "{(\"x1\",1):{'mean':y1.x2.mean(),'sd':y1.x1.std()},\n",
    " (\"x1\",0):{'mean':y0.x2.mean(),'sd':y0.x1.std()},\n",
    " (\"x2\",1):{'mean':y1.x1.mean(),'sd':y1.x2.std()},\n",
    " (\"x2\",0):{'mean':y0.x1.mean(),'sd':y0.x2.std()}\n",
    "}\n",
    "\n",
    "# Print\n",
    "pp.pprint(dist_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><center> Predicting a Single Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Pr(y == 1| X): 6.563751571296217e-07\n",
      "    Pr(y == 0| X): 5.55000404139091e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction for the 1 class\n",
    "a = st.norm(dist_locs[(\"x1\",1)]['mean'], dist_locs[(\"x1\",1)]['sd']).pdf(x1)\n",
    "b = st.norm(dist_locs[(\"x2\",1)]['mean'], dist_locs[(\"x2\",1)]['sd']).pdf(x2)\n",
    "c = pr_y1\n",
    "pr_1 = a * b * c \n",
    "\n",
    "# Prediction for the 0 class\n",
    "a = st.norm(dist_locs[(\"x1\",0)]['mean'], dist_locs[(\"x1\",0)]['sd']).pdf(x1)\n",
    "b = st.norm(dist_locs[(\"x2\",0)]['mean'], dist_locs[(\"x2\",0)]['sd']).pdf(x2)\n",
    "c = pr_y0\n",
    "pr_0 = a * b * c \n",
    "\n",
    "print(\n",
    "f'''\n",
    "    Pr(y == 1| X): {pr_1}\n",
    "    Pr(y == 0| X): {pr_0}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><center> Predicting Multiple Observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,dist_locs):\n",
    "    ''''''\n",
    "    store_preds = []\n",
    "    for i,row in data.iterrows():\n",
    "        \n",
    "        # Get the predictions using a Gaussan distribution\n",
    "        pr_0 = 1; pr_1 = 1\n",
    "        for j in range(1,len(row)):\n",
    "            \n",
    "            pr_0 *= st.norm(dist_locs[(row.index[j],0)]['mean'],\n",
    "                            dist_locs[(row.index[j],0)]['sd']).pdf(row.values[j])\n",
    "            pr_1 *= st.norm(dist_locs[(row.index[j],1)]['mean'], \n",
    "                            dist_locs[(row.index[j],1)]['sd']).pdf(row.values[j])\n",
    "        pr_0 *= pr_y0\n",
    "        pr_1 *= pr_y1\n",
    "        \n",
    "        # Assign the class designation to the highest probability\n",
    "        if pr_0 >= pr_1:\n",
    "            class_pred = 0\n",
    "        else:\n",
    "            class_pred = 1\n",
    "            \n",
    "        store_preds.append([pr_0,pr_1,class_pred])\n",
    "        \n",
    "    return pd.DataFrame(store_preds,columns=[\"pr_0\",\"pr_1\",\"pred\"])\n",
    "\n",
    "# Run\n",
    "preds_train = predict(train_drop,dist_locs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the predictive accuracy of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.749375"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictive accuracy of training data \n",
    "accuracy_train = sum(train_drop.y == preds_train.pred)/train_drop.shape[0]\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the predictive accuracy of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test test df \n",
    "preds_test = predict(test_drop, dist_locs)\n",
    "\n",
    "# Predictive accuracy of test data \n",
    "accuracy_train = sum(test_drop.y == preds_test.pred)/test_drop.shape[0]\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on age and income, the model predicted whether someone would vote or not with 70 percent accuracy. The test data was not as accurate as the training data (as is expected), but it is still more accurate than a coin flip. Likewise, using \"white\" vs \"non-white\" as a predictor, produced a model with 70 percent accuracy as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
